{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "3ApspCvo-3O_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tabulate import tabulate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"health.csv\")\n",
        "data_copy = data.copy()"
      ],
      "metadata": {
        "id": "9xtkUKw__C-q"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(columns=['Outcome'], axis=1)\n",
        "Y = data['Outcome']"
      ],
      "metadata": {
        "id": "E0V5G5LQAKZ4"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Processing Steps"
      ],
      "metadata": {
        "id": "XeguH3-SA1kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Check for any missing values"
      ],
      "metadata": {
        "id": "MERSnJtrA71w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the entire DataFrame\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "for column, count in missing_values.items():\n",
        "    if count > 0:\n",
        "        print(f'Column: {column}, Missing Values: {count}')\n"
      ],
      "metadata": {
        "id": "lM5j2rbPA3HN"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Drop the duplicates"
      ],
      "metadata": {
        "id": "23_pC1n0BBXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "UdHNBmTiBAov"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Normalization of the data"
      ],
      "metadata": {
        "id": "4FPVtfR3BHfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop('Outcome', axis=1)\n",
        "X = data.copy()\n",
        "\n",
        "num_data = X.select_dtypes(include=['number'])\n",
        "scaler = StandardScaler()\n",
        "\n",
        "data[num_data.columns] = scaler.fit_transform(data[num_data.columns])\n",
        "data = pd.concat([data, Y], axis=1)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "teu079FjBJQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "mKHX7oNlCnwA"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "y-4QYLU4CwQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "def cost(old_y, new_y):\n",
        "  return (np.sum(old_y*np.log(new_y) + (1 - old_y)*np.log(1 - new_y)))\n",
        "\n",
        "def predict(X, parameters):\n",
        "  return sigmoid(X.dot(parameters))\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    correct = np.sum(y_true == y_pred)\n",
        "    total = len(y_true)\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "MGFubtd6Cxtx"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_rate_schedule(iteration):\n",
        "    return 1.0 / (1.0 + iteration)\n",
        "\n",
        "alpha_values = [1.0, 0.1, 0.01, 0.001]\n",
        "results = []\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    # Reset initial_parameters and lists for each alpha value\n",
        "    initial_parameters = np.zeros(X.shape[1])\n",
        "    training_loss = []\n",
        "    validation_loss = []\n",
        "    training_accuracy = []\n",
        "    validation_accuracy = []\n",
        "\n",
        "    for i in range(0, 1000):\n",
        "        indices = np.random.permutation(len(X_train))\n",
        "        Shuf_X = X_train.iloc[indices]\n",
        "        Shuf_Y = Y_train.iloc[indices]\n",
        "\n",
        "        for j in range(len(Shuf_X)):\n",
        "            x_j = Shuf_X.iloc[j].values\n",
        "            y_j = Shuf_Y.iloc[j]\n",
        "\n",
        "            pred_y = (predict(x_j, initial_parameters) > 0.5)\n",
        "\n",
        "            gradient = x_j.T.dot(pred_y - y_j)\n",
        "            learning_rate = alpha * learning_rate_schedule(i)  # Apply the learning rate schedule\n",
        "            initial_parameters -= learning_rate * gradient\n",
        "\n",
        "        # Training Loss and Accuracy\n",
        "        Y_train_pred = predict(X_train, initial_parameters) > 0.5\n",
        "        training_loss.append(cost(Y_train, predict(X_train, initial_parameters)))\n",
        "        training_accuracy.append(accuracy(Y_train, Y_train_pred))\n",
        "\n",
        "        # Validation Loss and Accuracy\n",
        "        Y_val_pred = predict(X_val, initial_parameters) > 0.5\n",
        "        validation_loss.append(cost(Y_val, Y_val_pred))\n",
        "        validation_accuracy.append(accuracy(Y_val, Y_val_pred))\n",
        "\n",
        "    # Plot training loss vs. iteration\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.semilogy(training_loss, label='Training Loss')  # Use semilogy for exponential scale\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title(f'Training Loss vs. Iteration (alpha={alpha})')\n",
        "\n",
        "    # Plot validation loss vs. iteration\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.semilogy(validation_loss, label='Validation Loss', color='orange')  # Use semilogy\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title(f'Validation Loss vs. Iteration (alpha={alpha})')\n",
        "\n",
        "    # Plot training accuracy vs. iteration\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(training_accuracy, label='Training Accuracy', color='green')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title(f'Training Accuracy vs. Iteration (alpha={alpha})')\n",
        "\n",
        "    # Plot validation accuracy vs. iteration\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(validation_accuracy, label='Validation Accuracy', color='red')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title(f'Validation Accuracy vs. Iteration (alpha={alpha})')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    Y_test_pred = predict(X_test, initial_parameters) > 0.5\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(Y_test, Y_test_pred)\n",
        "\n",
        "    acc = accuracy_score(Y_test, Y_test_pred)\n",
        "\n",
        "    precision = precision_score(Y_test, Y_test_pred)\n",
        "\n",
        "    recall = recall_score(Y_test, Y_test_pred)\n",
        "\n",
        "    f1 = f1_score(Y_test, Y_test_pred)\n",
        "\n",
        "    results.append([alpha, acc, precision, recall, f1])\n",
        "\n",
        "    print(\"Confusion Matrix\")\n",
        "    print(cm)\n",
        "# Display the results table\n",
        "headers = [\"Alpha\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "print(tabulate(results, headers, tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "id": "ABM9EJGGDs9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing the sigmoid logistic function with tanh (tangent hyperbolic function)"
      ],
      "metadata": {
        "id": "34zJZIcyX2A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def derivative_tanh(x):\n",
        "  return 1 - (np.exp(2*x)/(np.exp(2*x)+1))\n",
        "\n",
        "def mean_squared_error(old_y, new_y):\n",
        "    n = len(old_y)\n",
        "    return (1/n)*np.sum((new_y - old_y)**2)"
      ],
      "metadata": {
        "id": "FWxvrmZkSTb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = []\n",
        "accuracy = []\n",
        "\n",
        "initial_parameters = np.zeros(X.shape[1])\n",
        "alpha = 0.1\n",
        "\n",
        "for i in range(0,1000):\n",
        "  x = X.dot(initial_parameters)\n",
        "  pred_y = tanh(x)"
      ],
      "metadata": {
        "id": "QChvXBsHYn2y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}